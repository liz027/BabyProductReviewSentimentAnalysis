{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c00c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for basic visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# for advanced visualizations\n",
    "import plotly.offline as py\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "init_notebook_mode(connected = True)\n",
    "import plotly.figure_factory as ff\n",
    "import pyrsm as rsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a6e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/amazon_baby.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c5b43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Planetwise Flannel Wipes</td>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>3</td>\n",
       "      <td>neutual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0                           Planetwise Flannel Wipes   \n",
       "1                              Planetwise Wipe Pouch   \n",
       "2                Annas Dream Full Quilt with 2 Shams   \n",
       "3  Stop Pacifier Sucking without tears with Thumb...   \n",
       "4  Stop Pacifier Sucking without tears with Thumb...   \n",
       "\n",
       "                                              review  rating sentiment  \n",
       "0  These flannel wipes are OK, but in my opinion ...       3   neutual  \n",
       "1  it came early and was not disappointed. i love...       5  positive  \n",
       "2  Very soft and comfortable and warmer than it l...       5  positive  \n",
       "3  This is a product well worth the purchase.  I ...       5  positive  \n",
       "4  All of my kids have cried non-stop when I trie...       5  positive  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classify the rating to different sentiment level. \n",
    "df['sentiment']=rsm.ifelse(df.rating>=4,'positive',rsm.ifelse(df.rating==3,'neutual','negative'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f53a35f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name         0\n",
       "review       0\n",
       "rating       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with Null value\n",
    "df=df.dropna(axis=0, how=\"any\", thresh=None, subset=None, inplace=False)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1459017d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182384, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f7ff60",
   "metadata": {},
   "source": [
    "# Phrase Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8433500",
   "metadata": {},
   "source": [
    "## 1. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed1a83bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c097ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['review'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a717d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5afa2d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    # get English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preprocessed_sentences = []\n",
    "    for i, row in df.iterrows():\n",
    "        sent = row[\"text\"]\n",
    "        sent = sent.lower()\n",
    "        words_list = sent.strip().split()\n",
    "        filtered_words = [ps.stem(word) for word in words_list if word not in stop_words and len(word) != 1] # also skip space from above translation\n",
    "        preprocessed_sentences.append(\" \".join(filtered_words))\n",
    "    df[\"text\"] = preprocessed_sentences\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92091dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9efbc905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "926ec70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7972b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f1c11d",
   "metadata": {},
   "source": [
    "### Divide text into phrases for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d0601bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagged_data = [[i.text for i in nlp(_d).noun_chunks] for i, _d in enumerate(train[\"text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "698e6c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"data/tagged_train_phrase.pkl\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(tagged_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7c180ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = pd.read_pickle('data/tagged_train_phrase.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4d11762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great video monitor, it works awesome and it has great reception from one room to another. great quality sound and video. good price\n"
     ]
    }
   ],
   "source": [
    "print(train.loc[131499,'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc8c644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great video monitor, work awesom great recept one room another. great qualiti sound video. good price\n"
     ]
    }
   ],
   "source": [
    "print(train.loc[131499,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f46ef61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great video monitor',\n",
       " 'awesom great recept',\n",
       " 'one room',\n",
       " 'great qualiti sound video',\n",
       " 'good price']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16094076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b445689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return word_counts, vocabulary, vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff2ce8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts, vocabulary, vocabulary_inv = build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fb93d720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "710431"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd840a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_data = [[vocabulary[word] for word in text] for text in tagged_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "635db41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gensim/similarities/__init__.py:15: UserWarning:\n",
      "\n",
      "The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0c063c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function used to learn word embeddings through Word2vec module\n",
    "def get_embeddings(inp_data, vocabulary_inv, size_features=100,\n",
    "                   mode='skipgram',\n",
    "                   min_word_count=2,\n",
    "                   context=5):\n",
    "    model_name = \"embedding\"\n",
    "    model_name = os.path.join(model_name)\n",
    "    num_workers = 15  # Number of threads to run in parallel\n",
    "    downsampling = 1e-3  # Downsample setting for frequent words\n",
    "    print('Training Word2Vec model...')\n",
    "    # use inp_data and vocabulary_inv to reconstruct sentences\n",
    "    sentences = [[vocabulary_inv[w] for w in s] for s in inp_data]\n",
    "    if mode == 'skipgram':\n",
    "        sg = 1\n",
    "        print('Model: skip-gram')\n",
    "    elif mode == 'cbow':\n",
    "        sg = 0\n",
    "        print('Model: CBOW')\n",
    "    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                        sg=sg,\n",
    "                                        vector_size=size_features,\n",
    "                                        min_count=min_word_count,\n",
    "                                        window=context,\n",
    "                                        sample=downsampling)\n",
    "    print(\"Saving Word2Vec model {}\".format(model_name))\n",
    "    embedding_weights = np.zeros((len(vocabulary_inv), size_features))\n",
    "    for i in range(len(vocabulary_inv)):\n",
    "        word = vocabulary_inv[i]\n",
    "        if word in embedding_model.wv:\n",
    "            embedding_weights[i] = embedding_model.wv[word]\n",
    "        else:\n",
    "            embedding_weights[i] = np.random.uniform(-0.25, 0.25,\n",
    "                                                     embedding_model.vector_size)\n",
    "    return embedding_weights,embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c191c36",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f26b0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n"
     ]
    }
   ],
   "source": [
    "embedding_weights_skipgram,model = get_embeddings(inp_data, vocabulary_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "11b87b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/embedding_weights_skipgram.pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(embedding_weights_skipgram, fp)\n",
    "with open(\"data/model.pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(model, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49e93d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights_skipgram = pd.read_pickle('data/embedding_weights_skipgram.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "00867c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66426"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.wv.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ee6a8a",
   "metadata": {},
   "source": [
    "### process training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bdba3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagged_train_data = tagged_data.copy()\n",
    "# tagged_test_data = [[i.text for i in nlp(_d).noun_chunks] for i, _d in enumerate(test[\"text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1d6bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/tagged_test_phrase.pkl\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(tagged_test_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c0d3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_train_data = tagged_data.copy()\n",
    "tagged_test_data = pd.read_pickle('data/tagged_test_phrase.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c09183",
   "metadata": {},
   "source": [
    "### Replace empty data by mean weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0a04d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = embedding_weights_skipgram.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cce04de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = []\n",
    "for doc in tagged_train_data:\n",
    "    vec = 0\n",
    "    length = 0\n",
    "    if len(doc)==0:\n",
    "        vec += avg\n",
    "    else:     \n",
    "        for w in doc:\n",
    "            try:\n",
    "                vec += embedding_weights_skipgram[vocabulary[w]]\n",
    "                length += 1\n",
    "            except:\n",
    "                continue\n",
    "        if length==0:\n",
    "            vec = avg\n",
    "    vec = vec / rsm.ifelse(length==0,1,length) \n",
    "    train_vec.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee3f0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = []\n",
    "for doc in tagged_test_data:\n",
    "    vec = 0\n",
    "    length = 0\n",
    "    if len(doc)==0:\n",
    "        vec += avg\n",
    "    else:     \n",
    "        for w in doc:\n",
    "            try:\n",
    "                vec += embedding_weights_skipgram[vocabulary[w]]\n",
    "                length += 1\n",
    "            except:\n",
    "                continue\n",
    "        if length==0:\n",
    "            vec = avg\n",
    "            \n",
    "    vec = vec / rsm.ifelse(length==0,1,length) \n",
    "    test_vec.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c3b688db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check = []\n",
    "\n",
    "# for i in range(len(test_vec)):\n",
    "#     if str(test_vec[i].shape) != '(100,)':\n",
    "#         check += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fdc7ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_vec\n",
    "X_test = test_vec\n",
    "y_train = train['sentiment']\n",
    "y_test = test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e11f9517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29948481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  1.4min remaining:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.6min finished\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegressionCV(cv=5,\n",
    "                               scoring='accuracy',\n",
    "                               random_state=42,\n",
    "                               n_jobs=-1,\n",
    "                               verbose=3,\n",
    "                               max_iter=1000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c07449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "487af14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.761219398525098"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a22517a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35373743337715996"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a2e3ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.761219398525098"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be4714d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6754736519553314"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test,clf.predict_proba(X_test),multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f0d0c",
   "metadata": {},
   "source": [
    "### Tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7d08bd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/joblib/externals/loky/process_executor.py:688: UserWarning:\n",
      "\n",
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(C=0.001, max_iter=100000, penalty='l1', solver='liblinear'),\n",
       " 'classifier__C': 0.001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'liblinear'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# # clf_new = LogisticRegression(max_iter=100000)\n",
    "# pipe = Pipeline([('classifier' , LogisticRegression())])\n",
    "\n",
    "# param_grid = [\n",
    "#     {'classifier' : [LogisticRegression(max_iter=100000)],\n",
    "#      'classifier__penalty' : ['l1','l2'],\n",
    "#     'classifier__C' : [0.001,0.01,0.1,1, 10, 20, 50],\n",
    "#     'classifier__solver':['liblinear']}\n",
    "# ]\n",
    "\n",
    "# clf_cv = GridSearchCV(pipe, param_grid = param_grid, cv=5,verbose=True, n_jobs=-1)\n",
    "# best_clf = clf_cv.fit(X_train, y_train)\n",
    "# best_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c6a39b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"data/best_clf.pkl\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(best_clf, fp)\n",
    "\n",
    "# pred = best_clf.predict(X_test)\n",
    "\n",
    "# best_clf.score(X_test, y_test)\n",
    "\n",
    "# f1_score(y_test, pred, average='macro')\n",
    "\n",
    "# roc_auc_score(y_test,best_clf.predict_proba(X_test),multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41364000",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa4e9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fa92032",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth = 8, random_state=42, max_leaf_nodes = 25, min_samples_leaf=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1c00dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier(max_depth=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b49fef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8b27fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fae043c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7640430956493133"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d53faab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29189111954872726"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred_rf, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8e0bac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7640430956493133"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred_rf, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "984c0f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6476475793271231"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test,rf_model.predict_proba(X_test),multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c104d759",
   "metadata": {},
   "source": [
    "### Tune Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4429dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 20, stop = 1000, num = 10)]\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# max_depth.append(None)\n",
    "# # # Minimum number of samples required to split a node\n",
    "# # min_samples_split = [2, 5, 10]\n",
    "# # # Minimum number of samples required at each leaf node\n",
    "# # min_samples_leaf = [1, 2, 4]\n",
    "# # Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_depth': max_depth,\n",
    "# #                'min_samples_split': min_samples_split,\n",
    "# #                'min_samples_leaf': min_samples_leaf\n",
    "#               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0ac4430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/joblib/externals/loky/process_executor.py:688: UserWarning:\n",
      "\n",
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# best_rf = rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d240f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"data/best_rf.pkl\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(best_rf, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be0fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_rf = best_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bfd803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22917c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score(y_test, pred_rf, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_score(y_test,rf_model.predict_proba(X_test),multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f99f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
